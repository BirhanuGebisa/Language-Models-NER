{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOoAYv5ThsHg"
      },
      "source": [
        "# News Classifier with Cohere"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVf0QmYJiJ22"
      },
      "source": [
        "With LLMs, instead of having to prepare thousands of training data points, you can get up and running with just a handful of examples, called *few-shot* classification. Having said that, you probably want to have a certain level of control over how you train a classifier, and especially, how to get the best performance out of a model. For example, if you do happen to have a large dataset at your disposal, you will want to make full use of it when training a classifier. With the Cohere API, we want to give this flexibility to developers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29uwe-jzJ9rh"
      },
      "outputs": [],
      "source": [
        "#! pip install cohere "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "y9-RyLu7KHII"
      },
      "outputs": [],
      "source": [
        "# Import the required modules\n",
        "import cohere\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os,sys\n",
        "sys.path.append('../scr')\n",
        "import config\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "AW4RsVSE4j74"
      },
      "outputs": [],
      "source": [
        "# Set up the Cohere client\n",
        "#api_key = 'apikey' # Paste your API key here. Remember to not share it publicly \n",
        "api_key = config.cohere_api[\"api_key\"]\n",
        "co = cohere.Client(api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laIkCcKa40PR"
      },
      "source": [
        "# Prepare the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gI7wMIdOrbiK",
        "outputId": "ebbaf0c0-60a2-487f-d1b3-7ec34175c35e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Domain</th>\n",
              "      <th>Title</th>\n",
              "      <th>Description</th>\n",
              "      <th>Body</th>\n",
              "      <th>Link</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>Analyst_Average_Score</th>\n",
              "      <th>Analyst_Rank</th>\n",
              "      <th>Reference_Final_Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rassegnastampa.news</td>\n",
              "      <td>Boris Johnson using a taxpayer-funded jet for ...</td>\n",
              "      <td>…often trigger a protest vote that can upset…t...</td>\n",
              "      <td>Boris Johnson using a taxpayer-funded jet for ...</td>\n",
              "      <td>https://rassegnastampa.news/boris-johnson-usin...</td>\n",
              "      <td>2021-09-09T18:17:46.258006</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>1.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>twitter.com</td>\n",
              "      <td>Stumbled across an interesting case, a woman f...</td>\n",
              "      <td>Stumbled across an interesting case, a woman f...</td>\n",
              "      <td>Stumbled across an interesting case, a woman f...</td>\n",
              "      <td>http://twitter.com/CoruscaKhaya/status/1435585...</td>\n",
              "      <td>2021-09-08T13:02:45.802298</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>12.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>atpe-tchad.info</td>\n",
              "      <td>Marché Résines dans les peintures et revêtemen...</td>\n",
              "      <td>…COVID-19…COVID…COVID…COVID-19 et Post COVID…C...</td>\n",
              "      <td>Le rapport d’étude de marché Résines dans les ...</td>\n",
              "      <td>http://atpe-tchad.info/2021/09/13/marche-resin...</td>\n",
              "      <td>2021-09-13T07:32:46.244403</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>badbluetech.bitnamiapp.com</td>\n",
              "      <td>AI drives data analytics surge, study finds</td>\n",
              "      <td>…hate raiders' linked to automated harassment ...</td>\n",
              "      <td>How to drive the funnel through content market...</td>\n",
              "      <td>http://badbluetech.bitnamiapp.com/p.php?sid=21...</td>\n",
              "      <td>2021-09-11T00:17:45.962605</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>6.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>kryptogazette.com</td>\n",
              "      <td>Triacetin Vertrieb Markt 2021: Globale Unterne...</td>\n",
              "      <td>…Abschnitten und Endanwendungen / Organisation...</td>\n",
              "      <td>Global Triacetin Vertrieb-Markt 2021 von Herst...</td>\n",
              "      <td>https://kryptogazette.com/2021/09/08/triacetin...</td>\n",
              "      <td>2021-09-08T12:47:46.078369</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0.13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Domain  \\\n",
              "0         rassegnastampa.news   \n",
              "1                 twitter.com   \n",
              "2             atpe-tchad.info   \n",
              "3  badbluetech.bitnamiapp.com   \n",
              "4           kryptogazette.com   \n",
              "\n",
              "                                               Title  \\\n",
              "0  Boris Johnson using a taxpayer-funded jet for ...   \n",
              "1  Stumbled across an interesting case, a woman f...   \n",
              "2  Marché Résines dans les peintures et revêtemen...   \n",
              "3        AI drives data analytics surge, study finds   \n",
              "4  Triacetin Vertrieb Markt 2021: Globale Unterne...   \n",
              "\n",
              "                                         Description  \\\n",
              "0  …often trigger a protest vote that can upset…t...   \n",
              "1  Stumbled across an interesting case, a woman f...   \n",
              "2  …COVID-19…COVID…COVID…COVID-19 et Post COVID…C...   \n",
              "3  …hate raiders' linked to automated harassment ...   \n",
              "4  …Abschnitten und Endanwendungen / Organisation...   \n",
              "\n",
              "                                                Body  \\\n",
              "0  Boris Johnson using a taxpayer-funded jet for ...   \n",
              "1  Stumbled across an interesting case, a woman f...   \n",
              "2  Le rapport d’étude de marché Résines dans les ...   \n",
              "3  How to drive the funnel through content market...   \n",
              "4  Global Triacetin Vertrieb-Markt 2021 von Herst...   \n",
              "\n",
              "                                                Link  \\\n",
              "0  https://rassegnastampa.news/boris-johnson-usin...   \n",
              "1  http://twitter.com/CoruscaKhaya/status/1435585...   \n",
              "2  http://atpe-tchad.info/2021/09/13/marche-resin...   \n",
              "3  http://badbluetech.bitnamiapp.com/p.php?sid=21...   \n",
              "4  https://kryptogazette.com/2021/09/08/triacetin...   \n",
              "\n",
              "                    timestamp  Analyst_Average_Score  Analyst_Rank  \\\n",
              "0  2021-09-09T18:17:46.258006                    0.0             4   \n",
              "1  2021-09-08T13:02:45.802298                    0.0             4   \n",
              "2  2021-09-13T07:32:46.244403                    0.0             4   \n",
              "3  2021-09-11T00:17:45.962605                    0.0             4   \n",
              "4  2021-09-08T12:47:46.078369                    0.0             4   \n",
              "\n",
              "   Reference_Final_Score  \n",
              "0                   1.96  \n",
              "1                  12.00  \n",
              "2                   0.05  \n",
              "3                   6.10  \n",
              "4                   0.13  "
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the dataset to a dataframe\n",
        "df = pd.read_csv('../data/Example_data.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10 entries, 0 to 9\n",
            "Data columns (total 9 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   Domain                 10 non-null     object \n",
            " 1   Title                  10 non-null     object \n",
            " 2   Description            10 non-null     object \n",
            " 3   Body                   10 non-null     object \n",
            " 4   Link                   10 non-null     object \n",
            " 5   timestamp              10 non-null     object \n",
            " 6   Analyst_Average_Score  10 non-null     float64\n",
            " 7   Analyst_Rank           10 non-null     int64  \n",
            " 8   Reference_Final_Score  10 non-null     float64\n",
            "dtypes: float64(2), int64(1), object(6)\n",
            "memory usage: 848.0+ bytes\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performing Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtaining Additional Stopwords From nltk\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gensim\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "# Removing Stopwords And Remove Words With 2 Or Less Characters\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2 and token not in stop_words:\n",
        "            result.append(token)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "from textblob import Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    Boris Johnson using a taxpayer-funded jet for ...\n",
              "1    Stumbled across an interesting case, a woman f...\n",
              "2    Le rapport d’étude de marché Résines dans le p...\n",
              "3    How to drive the funnel through content market...\n",
              "4    Global Triacetin Vertrieb-Markt 2021 von Herst...\n",
              "5    South African Police Service Office of the Pro...\n",
              "6    Today is the 7th anniversary [Tragic collapse ...\n",
              "7    Construction activity grew steadily by 4% in t...\n",
              "8    - Former Eskom CEO Matshela Moses Koko sought ...\n",
              "9    Global and Regional Beta-Carotene Market Resea...\n",
              "Name: Body, dtype: object"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Body']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /home/sucess/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Applying The Function To The Dataframe and Lower casing and removing punctuations\n",
        "# Lemmatization\n",
        "df['Body'] = df['Body'].astype(str).apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "df['Body'] = df['Body'].astype(str).apply(preprocess)\n",
        "#data_df['clean'] =\" \".join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(df) if w not in string.punctuation]))\n",
        "#Summary of title\n",
        "#lemmatization of Description \n",
        "df['Description'] = df['Description'].astype(str).apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "df['Description'] = df['Description'].astype(str).apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Writing a function to lemmatize words\n",
        "lmtzr = WordNetLemmatizer()\n",
        "def lem(text):\n",
        "    return [lmtzr.lemmatize(word) for word in text]\n",
        "\n",
        "# Applying the function to each row of the text\n",
        "# i.e. reducing each word to its lemma\n",
        "tokens_des = df['Description'].apply(lem)\n",
        "tokens_body = df['Body'].apply(lem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    [trigger, protest, vote, upset, minister, brea...\n",
              "1    [stumbled, interesting, case, woman, facing, e...\n",
              "2    [covid, covid, covid, covid, post, covid, covi...\n",
              "3    [hate, raider, linked, automated, harassment, ...\n",
              "4    [abschnitten, und, endanwendungen, organisatio...\n",
              "5    [crime, stamp, road, appear, court, sap, crime...\n",
              "6    [lagos, nigeria, south, african, killed, build...\n",
              "7    [additional, spending, building, repair, secur...\n",
              "8    [lawsuit, public, participation, designed, int...\n",
              "9    [key, player, dsm, basf, allied, biotech, chr,...\n",
              "Name: Description, dtype: object"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_des"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    [boris, johnson, taxpayer, funded, jet, electi...\n",
              "1    [stumbled, interesting, case, woman, facing, e...\n",
              "2    [rapport, étude, marché, résines, dans, peintu...\n",
              "3    [drive, funnel, content, marketing, link, buil...\n",
              "4    [global, triacetin, vertrieb, markt, von, hers...\n",
              "5    [south, african, police, service, office, prov...\n",
              "6    [today, anniversary, tragic, collapse, buildin...\n",
              "7    [construction, activity, grew, steadily, secon...\n",
              "8    [eskom, ceo, matshela, moses, koko, sought, da...\n",
              "9    [global, regional, beta, carotene, market, res...\n",
              "Name: Body, dtype: object"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    1120\n",
              "1      23\n",
              "2     861\n",
              "3    2316\n",
              "4     986\n",
              "5     156\n",
              "6     152\n",
              "7     259\n",
              "8     376\n",
              "9     489\n",
              "Name: Body, dtype: int64"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lengths = df['Body'].apply(lambda x: len(x)) \n",
        "lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    21\n",
              "1    23\n",
              "2    28\n",
              "3    30\n",
              "4    31\n",
              "5    26\n",
              "6    26\n",
              "7    21\n",
              "8    25\n",
              "9    28\n",
              "Name: Description, dtype: int64"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lengths = df['Description'].apply(lambda x: len(x)) \n",
        "lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    low\n",
              "1    low\n",
              "2    low\n",
              "3    low\n",
              "4    low\n",
              "5    low\n",
              "6    low\n",
              "7    low\n",
              "8    low\n",
              "9    low\n",
              "Name: rank_1, dtype: object"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rank__to_10 = df['Analyst_Average_Score'].apply(lambda x: 'low' if x < 5 else 'high')\n",
        "df['rank_1'] = rank__to_10\n",
        "df['rank_1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_sub_class(value):\n",
        "    if value >= 0 and value < 1:\n",
        "        return \"low_1\"\n",
        "    elif value >= 1 and value < 2:\n",
        "        return \"low_2\"\n",
        "    elif value >= 2 and value < 3:\n",
        "        return \"low_3\"\n",
        "    elif value >= 3 and value < 4:\n",
        "        return \"low_4\"\n",
        "    elif value >= 4 and value < 5:\n",
        "        return \"low_5\"\n",
        "    elif value >= 5 and value < 6:\n",
        "        return \"high_1\"\n",
        "    elif value >= 6 and value < 7:\n",
        "        return \"high_2\"\n",
        "    elif value >= 7 and value < 8:\n",
        "        return \"high_3\"\n",
        "    elif value >= 8 and value < 9:\n",
        "        return \"high_4\"\n",
        "    else:\n",
        "        return \"high_5\"\n",
        "\n",
        "#We will add new column with value 'high_1' for rows having Analyst_Average_Score value of 5-6, high_2 for rows having Analyst_Average_Score value of 6-7 til high_5 for rows having Analyst_Average_Score value of 9-10 \n",
        "#We will add new column with value 'low_1' for rows having Analyst_Average_Score value of 0-1, low_2 for rows having Analyst_Average_Score value of 1-2 til low_5 for rows having Analyst_Average_Score value of 4-5 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['rank_2'] = df['Analyst_Average_Score'].apply(lambda x: handle_sub_class(x))\n",
        "df['rank_3'] = df['Analyst_Average_Score'].apply(lambda x: handle_sub_class(int(\"{:.2f}\".format(x)[2])))\n",
        "df['rank_4'] = df['Analyst_Average_Score'].apply(lambda x: handle_sub_class(int(\"{:.2f}\".format(x)[3])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      low__low_1__low_1__low_1\n",
              "1      low__low_1__low_1__low_1\n",
              "2      low__low_1__low_1__low_1\n",
              "3      low__low_1__low_1__low_1\n",
              "4      low__low_1__low_1__low_1\n",
              "5      low__low_2__low_4__low_4\n",
              "6      low__low_1__low_1__low_1\n",
              "7    low__low_2__high_2__high_2\n",
              "8      low__low_1__low_4__low_4\n",
              "9      low__low_1__low_1__low_1\n",
              "Name: final_score, dtype: object"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['final_score'] = df['rank_1'].map(str)+'__'+df['rank_2'].map(str)+'__'+df['rank_3'].map(str)+'__'+df['rank_4'].map(str)\n",
        "df['final_score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "cM-2KGlQe80t"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and test portions\n",
        "# Training = For use in Sections 2 and 3\n",
        "# Test = For evaluating the classifier performance\n",
        "X,y = df['Title'], df['final_score']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=2, random_state=21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J-T5ZKP5s0t",
        "outputId": "41c946b1-9f46-44d0-bb49-31f348d252dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['low__low_2__high_2__high_2', 'low__low_1__low_1__low_1', 'low__low_2__low_4__low_4', 'low__low_1__low_4__low_4']\n"
          ]
        }
      ],
      "source": [
        "# View the list of all available categories\n",
        "intents = y_train.unique().tolist()\n",
        "print(intents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79RhgMQr452c"
      },
      "source": [
        "# 1 - Few-shot classification with the Classify endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-22ncqsi8sH"
      },
      "source": [
        "Few-shot here means we just need to supply a few examples per class and have a decent classifier working. With Cohere’s Classify endpoint, the ‘training’ dataset is referred to as *examples*. The minimum number of examples per class is five, where each example consists of a text (in our case, the `query`), and a label (in our case, the `label`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiHoK7A34-iJ"
      },
      "source": [
        "## Prepare the examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzWMEEb4LmRz",
        "outputId": "5a671716-d390-473f-856b-5c28453a4ece"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 4\n",
            "Total number of examples: 32\n"
          ]
        }
      ],
      "source": [
        "# Set the number of examples per category\n",
        "#EX_PER_CAT = 6\n",
        "\n",
        "# Create list of examples containing texts and labels - sample from the dataset\n",
        "ex_texts, ex_labels = [], []\n",
        "for intent in intents:\n",
        "  ex_texts += X_train.tolist()\n",
        "  ex_labels += y_train.tolist()\n",
        "\n",
        "print(f'Number of classes: {len(intents)}')\n",
        "print(f'Total number of examples: {len(ex_texts)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29oLOWrY6TnM"
      },
      "source": [
        "## Get classifications via the Classify endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "WujlVX-ERNW4"
      },
      "outputs": [],
      "source": [
        "# Collate the examples via the Example module\n",
        "from cohere.classify import Example\n",
        "\n",
        "examples = list()\n",
        "for txt, lbl in zip(ex_texts,ex_labels):\n",
        "  examples.append(Example(txt,lbl))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "NXCHB13-rbZZ"
      },
      "outputs": [],
      "source": [
        "# Perform classification\n",
        "def classify_text(text,examples):\n",
        "  classifications = co.classify(\n",
        "    model='medium', # model version - medium-22020720\n",
        "    inputs=[text],\n",
        "    examples=examples\n",
        "    )\n",
        "  return classifications.classifications[0].prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "6m331ZeKrbUB"
      },
      "outputs": [],
      "source": [
        "# Generate classification predictions on the test dataset (this will take a few minutes)\n",
        "y_pred = X_test.apply(classify_text, args=(examples,)).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVxRlIzR74Xz",
        "outputId": "86b085b0-b7cd-4994-e6cc-49b14e93c4d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 100.00\n",
            "F1-score: 100.00\n"
          ]
        }
      ],
      "source": [
        "# Compute metrics on the test dataset\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {100*accuracy:.2f}')\n",
        "print(f'F1-score: {100*f1:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-60rd2N1up2i"
      },
      "source": [
        "# 2 - Build your own classifier with the Embed endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbVkkSLXjaim"
      },
      "source": [
        "In this section, we’ll look at how we can use the Embed endpoint to build a classifier. We are going to build a classification model using these embeddings as inputs. For this, we’ll use the Support Vector Machine (SVM) algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YioSofGK7OAU"
      },
      "source": [
        "## Generate embeddings for the input text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "SAbu-iuGZg_3"
      },
      "outputs": [],
      "source": [
        "# Get embeddings\n",
        "def embed_text(text):\n",
        "  output = co.embed(\n",
        "                model='medium', # model version - medium-22020720\n",
        "                texts=text)\n",
        "  return output.embeddings\n",
        "\n",
        "# Embed and prepare the inputs\n",
        "X_train_emb = np.array(embed_text(X_train.tolist()))\n",
        "X_test_emb = np.array(embed_text(X_test.tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqt3Nutm7Gk3"
      },
      "source": [
        "## Get classifications via the SVM algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eS6_R6zZiuO",
        "outputId": "a618bb06-7ca1-4b03-fd6f-0a769d1e44a9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "SVC(class_weight='balanced')"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import modules\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Prepare the labels\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(y_train)\n",
        "y_train_le = le.transform(y_train)\n",
        "y_test_le = le.transform(y_test)\n",
        "\n",
        "# Initialize the model\n",
        "svm_classifier = SVC(class_weight='balanced')\n",
        "\n",
        "# Fit the training dataset to the model\n",
        "svm_classifier.fit(X_train_emb, y_train_le)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "uvY13RjmAXuX"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0])"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate classification predictions on the test dataset\n",
        "y_pred_le = svm_classifier.predict(X_test_emb)\n",
        "y_pred_le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EwE3VDZem-q",
        "outputId": "faafd49d-ca05-403a-ac02-9cb5d4a8c58b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 100.00\n",
            "F1-score: 100.00\n"
          ]
        }
      ],
      "source": [
        "# Compute metrics on the test dataset\n",
        "accuracy = accuracy_score(y_test_le, y_pred_le)\n",
        "f1 = f1_score(y_test_le, y_pred_le, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {100*accuracy:.2f}')\n",
        "print(f'F1-score: {100*f1:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNHwmhIzr8hv"
      },
      "source": [
        "# 3 - Finetuning a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkNhE3Kkj4XT"
      },
      "source": [
        "In this section, we build a custom model that’s finetuned to excel at a specific task, and potentially outperforming the previous two approaches we have seen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpgdrMbuPrZP"
      },
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "o95wRbeimrXC"
      },
      "outputs": [],
      "source": [
        "# Download the training dataset for finetuning\n",
        "df_train = pd.concat([X_train, y_train],axis=1)\n",
        "df_train.to_csv(\"news_finetune.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cIwcAUiRi5e"
      },
      "source": [
        "## Create a finetuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnFRzY19nvrT"
      },
      "source": [
        "Creating the finetune is done is the Playground. Refer to [this guide](https://docs.cohere.ai/finetuning-representation-models) for the finetuning steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJDDq9kdCMvH"
      },
      "source": [
        "## Get classifications via the Classify endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "Mq8sj5Fn0OuE"
      },
      "outputs": [],
      "source": [
        "# Perform classification using the finetuned model\n",
        "def classify_text_finetune(text):\n",
        "  classifications = co.classify(\n",
        "    model='eeba7d8c-61bd-42cd-a6b5-e31db27403cc-ft', # replace with your own finetune model ID \n",
        "    inputs=text\n",
        "    )\n",
        "  return classifications.classifications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T3JnGOK0OiZ"
      },
      "outputs": [],
      "source": [
        "# Generate classification predictions on the test dataset (this will take a few minutes)\n",
        "y_pred_raw = classify_text_finetune(X_test.tolist())\n",
        "y_pred = [y.prediction for y in y_pred_raw]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ961ixH9kbf",
        "outputId": "610b9ae4-3c62-4315-a572-ec1c81167eec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 94.50\n",
            "F1-score: 94.53\n"
          ]
        }
      ],
      "source": [
        "# Compute metrics on the test dataset\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {100*accuracy:.2f}')\n",
        "print(f'F1-score: {100*f1:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUOq3WzGkd9g"
      },
      "source": [
        "We have now seen how the different options compare performance-wise. And crucially, what’s important to note is the level of control that you have when working with the Classify endpoint."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('venv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "beb26789953ddcd0d4fafde83f340bee23aa0884c59e24dddb82f7574d20d741"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
